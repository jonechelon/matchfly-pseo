# GRU Airport Flight Scraper - Guia de Uso

## ğŸ“– VisÃ£o Geral

O **GRU Flight Scraper** Ã© um scraper profissional desenvolvido para extrair dados de voos do Aeroporto Internacional de Guarulhos (GRU). O scraper implementa mÃºltiplas estratÃ©gias para descobrir e utilizar endpoints de API oculta, sem necessidade de Selenium.

## âœ¨ CaracterÃ­sticas

### ğŸ” Descoberta Inteligente de API
- Tenta mÃºltiplos endpoints comuns de API
- Parseia dados JSON embutidos no HTML quando necessÃ¡rio
- Se a coleta falhar: retorna lista vazia e registra erro crÃ­tico (sem dados fake)

### ğŸ›¡ï¸ Tratamento Robusto de Erros
- Try-catch em todos os pontos crÃ­ticos
- Logging detalhado de todos os erros
- Graceful degradation (continua mesmo com falhas parciais)

### ğŸ“Š Filtragem Inteligente
- Filtra voos **Cancelados**
- Filtra voos **Atrasados** (atraso > 2 horas)
- Calcula atraso em horas automaticamente

### ğŸ“ Logging Completo
- Logs no console (stdout)
- Logs em arquivo (`gru_scraper.log`)
- Diferentes nÃ­veis: INFO, WARNING, ERROR, DEBUG

## ğŸš€ Como Usar

### 1. InstalaÃ§Ã£o das DependÃªncias

```bash
cd ~/matchfly

# Criar ambiente virtual
python3 -m venv venv

# Ativar ambiente virtual
source venv/bin/activate  # Linux/macOS
# ou
venv\Scripts\activate     # Windows

# Instalar dependÃªncias
pip install -r requirements.txt
```

### 2. Executar o Scraper

**MÃ©todo 1: Script Runner (Recomendado)**
```bash
python3 run_gru_scraper.py
```

**MÃ©todo 2: Diretamente**
```bash
python3 src/scrapers/gru_flights_scraper.py
```

**MÃ©todo 3: Como MÃ³dulo**
```bash
python3 -m src.scrapers.gru_flights_scraper
```

### 3. Usar Programaticamente

```python
from src.scrapers.gru_flights_scraper import GRUFlightScraper

# Criar instÃ¢ncia do scraper
scraper = GRUFlightScraper(output_file="data/flights-db.json")

# Executar scraping completo
scraper.run()

# Ou usar mÃ©todos individuais
flights = scraper.fetch_flights()
filtered = scraper.filter_flights(flights)
scraper.save_to_json(filtered)
```

## ğŸ“ Arquivos Gerados

### `data/flights-db.json`
Arquivo principal com os dados dos voos:

```json
{
  "metadata": {
    "source": "GRU Airport (gru.com.br)",
    "scraped_at": "2026-01-11T18:34:35.005828",
    "total_flights": 5,
    "filters": "Cancelados ou Atrasados > 2h"
  },
  "flights": [
    {
      "flight_number": "LA3090",
      "airline": "LATAM",
      "scheduled_time": "2026-01-11 15:34:34",
      "actual_time": "2026-01-11 18:04:34",
      "status": "Atrasado",
      "delay_hours": 2.5
    }
  ]
}
```

### `gru_scraper.log`
Arquivo de log com histÃ³rico de execuÃ§Ãµes:

```
2026-01-11 18:34:34,243 - scrapers.gru_flights_scraper - INFO - ğŸš€ GRU Airport Flight Scraper - Iniciando
2026-01-11 18:34:34,246 - scrapers.gru_flights_scraper - INFO - ğŸ” Iniciando descoberta de API endpoints...
2026-01-11 18:34:35,008 - scrapers.gru_flights_scraper - INFO - âœ… Scraping concluÃ­do com sucesso!
```

## ğŸ”§ ConfiguraÃ§Ã£o AvanÃ§ada

### Personalizar Output

```python
# Mudar caminho do arquivo de saÃ­da
scraper = GRUFlightScraper(output_file="custom/path/flights.json")

# Modificar filtros
def custom_filter(flight):
    return flight['delay_hours'] > 3  # Apenas atrasos > 3h

all_flights = scraper.fetch_flights()
custom_filtered = [f for f in all_flights if custom_filter(f)]
scraper.save_to_json(custom_filtered)
```

### Adicionar Novos Endpoints

Edite a lista `API_ENDPOINTS` na classe:

```python
API_ENDPOINTS = [
    "/pt-br/api/voos/partidas",
    "/seu/novo/endpoint",
]
```

### Ajustar Logging

```python
import logging

# Mudar nÃ­vel de log para DEBUG
logging.getLogger('scrapers.gru_flights_scraper').setLevel(logging.DEBUG)

# Desabilitar logs no console
logger.handlers = [h for h in logger.handlers if not isinstance(h, logging.StreamHandler)]
```

## ğŸ—ï¸ Arquitetura do CÃ³digo

### Classes Principais

#### `GRUFlightScraper`
Classe principal que gerencia todo o processo de scraping.

**MÃ©todos PÃºblicos:**
- `run()` - Executa o scraper completo
- `fetch_flights()` - Busca dados de voos
- `filter_flights(flights)` - Filtra voos por critÃ©rios
- `save_to_json(flights)` - Salva dados em JSON

**MÃ©todos Internos:**
- `_extract_next_build_id(html)` - Captura o `buildId` do Next.js no HTML
- `fetch_next_data_endpoint(build_id)` - Usa `/_next/data/{buildId}/...` para buscar JSON
- `_filter_only_today(flights)` - MantÃ©m apenas voos com data de hoje
- `discover_api_endpoint()` - Descobre endpoints vÃ¡lidos
- `scrape_html_fallback()` - Fallback quando API nÃ£o estÃ¡ disponÃ­vel
- `_parse_flight(flight_data)` - Parseia dados individuais de voo
- `_parse_datetime(dt_string)` - Parseia strings de data/hora
- `_parse_embedded_data(data)` - Extrai dados JSON do HTML

### Fluxo de ExecuÃ§Ã£o

```
1. InicializaÃ§Ã£o
   â””â”€ Configura headers e sessÃ£o HTTP

2. Descoberta de API
   â”œâ”€ Testa endpoints conhecidos
   â”œâ”€ Valida respostas JSON
   â””â”€ Retorna primeiro endpoint vÃ¡lido

3. Coleta de Dados
   â”œâ”€ SessÃ£o com Cloudscraper (cookies/JS challenges automaticamente)
   â”œâ”€ Carrega `/pt-br/voos` para obter `buildId` e tenta `/_next/data/{buildId}/pt-br/voos.json`
   â”œâ”€ Se API encontrada: usa endpoint
   â”œâ”€ Se API nÃ£o encontrada: parseia HTML
   â””â”€ Se falha total: retorna lista vazia e registra erro crÃ­tico (sem dados fake)

4. Processamento
   â”œâ”€ Normaliza dados de voos
   â”œâ”€ Calcula atrasos
   â””â”€ Parseia horÃ¡rios

4b. Filtro de Data
   â””â”€ MantÃ©m apenas voos com data de hoje para evitar persistÃªncia de voos antigos

5. Filtragem
   â”œâ”€ Identifica voos cancelados
   â”œâ”€ Identifica voos atrasados > 2h
   â””â”€ Retorna lista filtrada

6. PersistÃªncia
   â”œâ”€ Adiciona metadados
   â”œâ”€ Formata JSON com indentaÃ§Ã£o
   â””â”€ Salva em arquivo
```

## ğŸ› Troubleshooting

### Erro: ModuleNotFoundError

**Causa:** DependÃªncias nÃ£o instaladas

**SoluÃ§Ã£o:**
```bash
pip install -r requirements.txt
```

### Erro: Permission Denied ao salvar JSON

**Causa:** Pasta `data/` nÃ£o existe ou sem permissÃ£o

**SoluÃ§Ã£o:**
```bash
mkdir -p data
chmod 755 data
```

### Warning: urllib3 OpenSSL

**Causa:** VersÃ£o antiga do OpenSSL/LibreSSL

**SoluÃ§Ã£o:** NÃ£o afeta funcionalidade, mas pode atualizar:
```bash
pip install --upgrade urllib3
```

### Nenhum dado extraÃ­do

**Causa:** Site mudou estrutura ou API indisponÃ­vel

**SoluÃ§Ã£o:**
1. Verifique logs para detalhes
2. Atualize endpoints na lista `API_ENDPOINTS`
3. Use modo DEBUG para anÃ¡lise:
   ```python
   logger.setLevel(logging.DEBUG)
   ```

### Rate Limiting / Bloqueio

**Causa:** Muitas requisiÃ§Ãµes em curto perÃ­odo

**SoluÃ§Ã£o:** Adicione delays entre requisiÃ§Ãµes:
```python
import time
time.sleep(2)  # 2 segundos entre requisiÃ§Ãµes
```

## ğŸ“ˆ Performance

### MÃ©tricas TÃ­picas
- **Tempo de execuÃ§Ã£o:** ~1-5 segundos
- **RequisiÃ§Ãµes HTTP:** 3-10 (dependendo de endpoints testados)
- **MemÃ³ria:** < 50MB
- **Tamanho do arquivo JSON:** ~1-10KB (varia com nÃºmero de voos)

### OtimizaÃ§Ãµes Implementadas
- âœ… SessÃ£o HTTP reutilizÃ¡vel
- âœ… Timeout em todas as requisiÃ§Ãµes
- âœ… Lazy evaluation de dados
- âœ… Early return em descoberta de API

## ğŸ” SeguranÃ§a

### PrÃ¡ticas Implementadas
- âœ… ValidaÃ§Ã£o de dados de entrada
- âœ… SanitizaÃ§Ã£o de strings
- âœ… Headers de User-Agent realista
- âœ… Timeouts para prevenir hanging
- âœ… Sem credenciais hardcoded

### RecomendaÃ§Ãµes
- ğŸ”¸ Respeite robots.txt do site
- ğŸ”¸ Implemente rate limiting em produÃ§Ã£o
- ğŸ”¸ Use proxy se necessÃ¡rio
- ğŸ”¸ Monitore logs de erro

## ğŸš€ PrÃ³ximos Passos

### Melhorias Sugeridas
- [ ] Implementar cache de requisiÃ§Ãµes
- [ ] Adicionar suporte a proxy
- [ ] Criar scheduler para execuÃ§Ã£o automÃ¡tica
- [ ] Adicionar testes unitÃ¡rios
- [ ] Implementar retry com backoff exponencial
- [ ] Adicionar suporte a mÃºltiplos aeroportos
- [ ] Criar API REST para consumir dados
- [ ] Dashboard web para visualizaÃ§Ã£o

### IntegraÃ§Ãµes PossÃ­veis
- ğŸ“§ NotificaÃ§Ãµes por email (voos cancelados)
- ğŸ’¬ Bot Telegram/WhatsApp
- ğŸ“Š Dashboard Grafana
- ğŸ”” Alertas em tempo real
- ğŸ—„ï¸ Banco de dados (PostgreSQL/MongoDB)

## ğŸ“š Recursos Adicionais

### DocumentaÃ§Ã£o
- [BeautifulSoup4 Docs](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)
- [Cloudscraper](https://github.com/VeNoMouS/cloudscraper)
- [Python Logging](https://docs.python.org/3/library/logging.html)

### Ferramentas Ãšteis
- **Insomnia/Postman:** Testar APIs manualmente
- **Chrome DevTools:** Inspecionar chamadas de rede
- **jq:** Processar JSON na linha de comando

## ğŸ‘¥ Suporte

Para dÃºvidas ou problemas:
1. Verifique os logs em `gru_scraper.log`
2. Consulte esta documentaÃ§Ã£o
3. Abra uma issue no repositÃ³rio

---

**VersÃ£o:** 1.0.0  
**Ãšltima AtualizaÃ§Ã£o:** 2026-01-11  
**LicenÃ§a:** MIT

