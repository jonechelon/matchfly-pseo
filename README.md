# MatchFly

## VisÃ£o Geral

MatchFly Ã© uma plataforma automatizada de agregaÃ§Ã£o e anÃ¡lise de status de voos, desenvolvida com foco em escalabilidade, manutenibilidade e resoluÃ§Ã£o de problemas via Ã³rgÃ£os confiÃ¡veis como a ANAC. O sistema realiza web scraping de mÃºltiplas fontes, processa status e gera pÃ¡ginas estÃ¡ticas otimizadas para SEO.

## Arquitetura

### Estrutura do Projeto

```
matchfly/
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/          # Pipelines CI/CD (GitHub Actions)
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrapers/           # MÃ³dulos de web scraping
â”‚   â””â”€â”€ templates/          # Templates Jinja2 para geraÃ§Ã£o de HTML
â”œâ”€â”€ data/                   # Armazenamento de dados processados (JSON)
â”œâ”€â”€ public/                 # Arquivos estÃ¡ticos gerados
â”œâ”€â”€ requirements.txt        # DependÃªncias Python
â””â”€â”€ README.md              # DocumentaÃ§Ã£o tÃ©cnica
```

### Stack TecnolÃ³gico

- **Python 3.9+**: Linguagem principal
- **BeautifulSoup4**: Parsing de HTML/XML para web scraping
- **Requests**: Cliente HTTP para requisiÃ§Ãµes web
- **Jinja2**: Engine de templates para geraÃ§Ã£o de HTML
- **Python-Slugify**: GeraÃ§Ã£o de URLs amigÃ¡veis (SEO)

## Funcionalidades Principais

### 1. Web Scraping Modular
- Arquitetura baseada em scrapers independentes
- Suporte para mÃºltiplas fontes de dados
- Rate limiting e retry logic integrados
- Tratamento robusto de erros

### 2. Processamento de Dados
- NormalizaÃ§Ã£o e validaÃ§Ã£o de dados esportivos
- Armazenamento em JSON estruturado
- Cache inteligente para otimizaÃ§Ã£o de performance

### 3. GeraÃ§Ã£o de PÃ¡ginas EstÃ¡ticas
- Templates Jinja2 responsivos
- SEO-friendly URLs (usando slugify)
- OtimizaÃ§Ã£o para performance web
- Estrutura preparada para CDN

### 4. CI/CD
- Workflows automatizados via GitHub Actions
- Testes automatizados
- Deploy contÃ­nuo

## InstalaÃ§Ã£o

### PrÃ©-requisitos

- Python 3.9 ou superior
- pip (gerenciador de pacotes Python)
- Git

### Setup Local

```bash
# Clone o repositÃ³rio
git clone <repository-url>
cd matchfly

# Crie e ative o ambiente virtual
python3 -m venv venv
source venv/bin/activate  # Linux/macOS
# ou
venv\Scripts\activate     # Windows

# Instale as dependÃªncias
pip install -r requirements.txt
```

## Uso

### Scrapers DisponÃ­veis

#### ðŸ›« GRU Airport Flight Scraper

Scraper profissional para voos do Aeroporto de Guarulhos com descoberta automÃ¡tica de API.

```bash
# Executar scraper GRU
python3 run_gru_scraper.py

# Ou com exemplos interativos
python3 examples/example_usage.py
```

**CaracterÃ­sticas:**
- âœ… Descoberta inteligente de API endpoints
- âœ… Filtros: Cancelados ou Atrasados > 2h
- âœ… Logging robusto (console + arquivo)
- âœ… Tratamento completo de erros
- âœ… Output: `data/flights-db.json`

**Uso ProgramÃ¡tico:**

```python
from src.scrapers import GRUFlightScraper

# Criar scraper
scraper = GRUFlightScraper(output_file="data/flights-db.json")

# Executar
scraper.run()

# Ou usar mÃ©todos individuais
flights = scraper.fetch_flights()
filtered = scraper.filter_flights(flights)
scraper.save_to_json(filtered)
```

ðŸ“– [DocumentaÃ§Ã£o Completa do GRU Scraper](docs/GRU_SCRAPER_USAGE.md)

### Executando Scrapers (Exemplo GenÃ©rico)

```python
# Exemplo bÃ¡sico de uso
from src.scrapers import match_scraper

# Executar scraping
matches = match_scraper.fetch_matches()
```

### Gerando PÃ¡ginas

```python
# Exemplo de geraÃ§Ã£o de pÃ¡ginas
from src.templates import renderer

# Renderizar template
renderer.generate_match_page(match_data)
```

## Estrutura de Dados

### Formato JSON (data/)

```json
{
  "match_id": "unique-id",
  "home_team": "Team A",
  "away_team": "Team B",
  "date": "2026-01-11T20:00:00Z",
  "competition": "League Name",
  "status": "scheduled|live|finished"
}
```

## Desenvolvimento

### Boas PrÃ¡ticas

1. **CÃ³digo Limpo**: Seguir PEP 8 (Python)
2. **Type Hints**: Usar anotaÃ§Ãµes de tipo
3. **Docstrings**: Documentar funÃ§Ãµes e classes
4. **Testes**: Cobertura mÃ­nima de 80%
5. **Git Flow**: Feature branches + Pull Requests

### Estrutura de Commit

```
<tipo>(<escopo>): <descriÃ§Ã£o curta>

<descriÃ§Ã£o detalhada opcional>
```

Tipos: `feat`, `fix`, `docs`, `refactor`, `test`, `chore`

### Criando um Novo Scraper

```python
# src/scrapers/new_source_scraper.py
from typing import List, Dict
import requests
from bs4 import BeautifulSoup

class NewSourceScraper:
    """Scraper para [Nome da Fonte]."""
    
    BASE_URL = "https://example.com"
    
    def fetch_matches(self) -> List[Dict]:
        """
        Busca partidas da fonte.
        
        Returns:
            Lista de dicionÃ¡rios com dados das partidas
        """
        response = requests.get(self.BASE_URL)
        soup = BeautifulSoup(response.content, 'html.parser')
        # Implementar lÃ³gica de scraping
        return matches
```

## CI/CD Pipeline

### GitHub Actions Workflow

```yaml
# .github/workflows/main.yml
name: CI/CD Pipeline

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v2
      - name: Set up Python
        uses: actions/setup-python@v2
        with:
          python-version: '3.9'
      - name: Install dependencies
        run: pip install -r requirements.txt
      - name: Run tests
        run: pytest
```

## Performance

### OtimizaÃ§Ãµes Implementadas

- **Caching**: ReduÃ§Ã£o de requisiÃ§Ãµes redundantes
- **Async I/O**: Para operaÃ§Ãµes de rede (futuro)
- **Lazy Loading**: Carregamento sob demanda
- **Compression**: Gzip para arquivos estÃ¡ticos

## SeguranÃ§a

- ValidaÃ§Ã£o de input em todos os scrapers
- SanitizaÃ§Ã£o de dados antes do processamento
- Rate limiting para evitar bloqueios
- Sem armazenamento de credenciais em cÃ³digo

## Funcionalidades Implementadas âœ…

### ðŸ›« GRU Airport Flight Scraper
- âœ… Descoberta automÃ¡tica de API endpoints
- âœ… Filtros: Cancelados ou Atrasados > 2h
- âœ… Logging robusto (console + arquivo)
- âœ… Tratamento completo de erros
- âœ… Output estruturado em JSON

### ðŸŽ¨ Gerador de PÃ¡ginas EstÃ¡ticas
- âœ… Template CRO-optimized (tier2-anac400)
- âœ… ValidaÃ§Ã£o de affiliate link obrigatÃ³ria
- âœ… CÃ¡lculo automÃ¡tico de "hours_ago"
- âœ… Slugs SEO-friendly
- âœ… Schemas JSON-LD (BroadcastEvent + FAQ)
- âœ… Checkboxes interativos com JavaScript
- âœ… Tabela de direitos ANAC
- âœ… Design mobile-first (Tailwind CSS)

### ðŸ“Š Pipeline Completo
- âœ… Script `run_pipeline.sh` (scraping â†’ geraÃ§Ã£o)
- âœ… Exemplos prÃ¡ticos interativos
- âœ… DocumentaÃ§Ã£o completa

## Roadmap Futuro

- [ ] Scrapers para outros aeroportos (CGH, BSB, SDU, GIG)
- [ ] Sistema de notificaÃ§Ãµes em tempo real
- [ ] API REST para consumo de dados
- [ ] Dashboard administrativo
- [ ] Templates adicionais (Tier 1, Tier 3)
- [ ] Suporte multi-idioma
- [ ] Mobile app

## Contribuindo

1. Fork o projeto
2. Crie uma branch (`git checkout -b feature/nova-funcionalidade`)
3. Commit suas mudanÃ§as (`git commit -m 'feat: adiciona nova funcionalidade'`)
4. Push para a branch (`git push origin feature/nova-funcionalidade`)
5. Abra um Pull Request

## LicenÃ§a

[Definir LicenÃ§a - MIT, Apache 2.0, etc.]

## Contato

- **Projeto**: MatchFly
- **Maintainer**: [Seu Nome/Equipe]
- **Email**: [contato@matchfly.com]

## Troubleshooting

### Problemas Comuns

**1. Erro de dependÃªncias**
```bash
pip install --upgrade pip
pip install -r requirements.txt --force-reinstall
```

**2. Scraper retorna vazio**
- Verificar se o site alvo mudou estrutura HTML
- Confirmar conectividade de rede
- Checar rate limiting

**3. Erro de encoding**
```python
# Use UTF-8 explicitamente
with open('file.json', 'w', encoding='utf-8') as f:
    json.dump(data, f, ensure_ascii=False)
```

## Status do Projeto

**VersÃ£o Atual**: 0.1.0 (Desenvolvimento Inicial)

**Status**: ðŸš§ Em Desenvolvimento Ativo

